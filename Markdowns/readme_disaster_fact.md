# â­ Spark ETL â€“ Kreiranje `Disaster_FACT` tablice

## ğŸ¯ Cilj
Faktâ€‘tablica **Disaster_FACT** povezuje sve dimenzije (State, Disaster, Declaration, Incident_Dates) i sprema dvije mjere:

* `deaths` â€“ broj smrtnih sluÄajeva
* (implicitno) broj redova = broj deklaracija

Izvori podataka:
* **80â€¯%** â€“ veÄ‡ uÄitani OLTP podaci u bazi (`State`, `Disaster`, `Declaration`)
* **20â€¯%** â€“ preostali CSV `US_DISASTERS_PROCESSED_20.csv`

Ukupan oÄekivani broj redova nakon spajanja â‰ˆ **55â€¯684** (80â€¯%Â +Â 20â€¯%).

---

## ğŸ“‚ Datoteka: `fact_table.py`

### 1. Unicode izlaz
```python
sys.stdout.reconfigure(encoding="utf-8")
```
Kako bi terminal pravilno prikazivao hrvatske znakove.

---

### 2. Spark helper
```python
SparkSession.builder \
    .appName("US_DISASTER_SCHEMA") \
    .config("spark.sql.shuffle.partitions", "4") \
    .getOrCreate()
```
ÄŒetiri shuffleâ€‘particije su OK za ~60â€¯k redova.

---

## ğŸ—ï¸ DioÂ A â€“ Funkcija `make_staging_fact()`

```python
csv_df  = spark.read.csv(csv_path, header=True, inferSchema=True) ...
```
* Iz CSVâ€‘a uzima samo potrebne stupce + **`country_name`** (county) â€“ bitno za mapiranje u `State_DIM`.

```python
decl = spark.read.jdbc(... '"Declaration"' ...)
state = spark.read.jdbc(... '"State"' ...)
disaster = spark.read.jdbc(... '"Disaster"' ...)
```
* ÄŒitamo OLTP tablice i mapiramo `state_fk` i `disaster_fk` na stvarne vrijednosti.

```python
fact80 = decl.join(state, "state_fk") \
           .join(disaster, "disaster_fk") ...
```
Dobijamo **80Â %** staging set s istim kolonama kao `csv_df`.

```python
staging_fact = fact80.unionByName(csv_df)
```
Rezultat â‰ˆÂ 55â€¯684 redova:
* Duplikati su dozvoljeni (svaka deklaracija se zadrÅ¾ava).

---

## ğŸ—ï¸ DioÂ B â€“ Funkcija `build_fact()`

### 1. UÄitavanje dimenzija
```python
state_dim      = spark.read.jdbc(... "state_dim"      ...)
disaster_dim   = spark.read.jdbc(... "disaster_dim"   ...)
declaration_dim = spark.read.jdbc(... "declaration_dim" ...)
dates_dim      = spark.read.jdbc(... "incident_datesdim" ...)
```
Svaka dimenzija sadrÅ¾i **surrogate kljuÄ** + prirodni kljuÄ(eve) potrebne za join.

### 2. JOIN logika
```python
coalesce(col("s.state_tk"), UNKNOWN).alias("state_tk")
```
* Ako ne postoji match â†’ koristi `0` (UNKNOWN). Time **nikad** ne gubimo redove.

| Redoslijed joinâ€‘ova | KljuÄ                                    |
|-------------------- | ---------------------------------------- |
| **State**           | `country_name` (u velikim slovima)      |
| **Disaster**        | `disaster_number`                       |
| **Declaration**     | `declaration_request_number`            |
| **Incident_Dates**  | `(incident_begin_date, incident_end_date)` |

### 3. Odabir mjera
```python
.select(..., col("f.deaths").alias("deaths"))
```
Za sada imamo samo jednu mjeruÂ â€“ `deaths`.

### 4. Surrogate kljuÄ faktâ€‘reda
```python
row_number().over(Window.orderBy("state_tk", "disaster_tk", "declaration_tk", "incident_dates_tk"))
```
DeterministiÄki generira `disaster_fact_tk`.

---

## ğŸ’¾ Pisanje u bazu
```python
fact_df.write.jdbc(url, "disaster_fact", mode="overwrite", properties=props)
```
Tablica se potpuno prepiÅ¡e pri svakom pokretanju ETLâ€‘a (SCDâ€‘typeÂ 0 za fact).

---

## ğŸ“Š OÄekivani output
* `Staging_fact rows:` â‰ˆÂ 55â€¯684
* `Disaster_FACT rows:` jednako stagingu (jer UNKNOWN spreÄava gubitke)

---

## ğŸ“ Bestâ€‘practice napomene
1. Ako dodaÅ¡ nove mjere (npr. `injuries`, `damage_usd`) â€“ samo proÅ¡iri `.select()` u `build_fact()`.
2. Za velike clustere poveÄ‡aj `spark.sql.shuffle.partitions` kako bi raspodijelio workload.
3. **Modeliranje**: ovu tablicu moÅ¾eÅ¡ agregirati na razine (state, godina, incident_type) za OLAP izvjeÅ¡tavanje.

---

**Autor**: Data Engineering pomoÄ‡nik Â· lipanjÂ 2025.

