# ğŸŒªï¸ Spark ETL â€“Â Kreiranje `Disaster_DIM` dimenzijske tablice

## ğŸ¯ Cilj
Dimenzija **Disaster_DIM** opisuje svaki FEMA â€disasterâ€œ (katastrofu) i koje su vrste federalnih programa (IA, PA,Â â€¦) aktivirane.

| Stupac                 | Opis                                                       |
|------------------------|------------------------------------------------------------|
| `disaster_tk`          | Surrogate kljuÄ (1Â â€¦Â n)                                    |
| `version`              | SCDâ€‘verzija (trenutno 1)                                   |
| `date_from`            | Datum kada red postaje aktivan                             |
| `date_to`              | Datum kraja (NULLÂ = joÅ¡ aktivan)                           |
| `disaster_number`      | **Prirodni kljuÄ** â€“ FEMA ID (jedinstven)                  |
| `incident_type`        | Vrsta incidenta (Tornado, Blizzard, Hurricane, â€¦)          |
| `*_program_declared`   | ÄŒetiri booleana: IH, IA,Â PA,Â HM                             |

> `disaster_tk = 0` rezerviran je za **UNKNOWN** red, da se pri LEFTâ€‘JOINâ€‘u ne gube Äinjenice.

---

## ğŸ—‚ï¸ Struktura skripte

### 1.â€¯Unicode izlaz
```python
sys.stdout.reconfigure(encoding="utf-8")
```
Za siguran prikaz hrvatskih znakova u Windows/VSÂ Code terminalu.

---

### 2.â€¯Spark helper
```python
def get_spark_session():
    return (SparkSession.builder
            .appName("US_DISASTER_SCHEMA")
            .config("spark.sql.shuffle.partitions", "4")
            .getOrCreate())
```
- 4 shuffleâ€‘particije â€“Â dovoljno za
> 8Â k redova; manji overhead.

---

### 3.â€¯Transform funkcija `transform_disaster_dim()`

#### 3.1 Extract & casting
```python
BOOL_COLS = ["ih_program_declared", "ia_program_declared", "pa_program_declared", "hm_program_declared"]

def cast_bool(df):
    sel = [col("disaster_number"), col("incident_type")]
    sel += [col(c).cast("boolean").alias(c) for c in BOOL_COLS]
    return df.select(*sel)

db_df  = cast_bool(spark.read.jdbc(... '"Disaster"' ...))
csv_df = cast_bool(spark.read.csv(csv_path, header=True, inferSchema=True))
```
- Iz baze (80â€¯%) i CSVâ€‘a (20â€¯%) Äitamo potpuno istu strukturu.
- Svi boolean stupci konvertirani su `cast("boolean")` kako bi `unionByName` bio siguran.

#### 3.2 Union + dedup
```python
all_disasters = (db_df.unionByName(csv_df)
                 .dropna(subset=["disaster_number"])
                 .dropDuplicates(["disaster_number"]))
```
- JamÄimo **jedan red za svaki `disaster_number`** (prirodni kljuÄ).

#### 3.3 Surrogate + SCD
```python
w = Window.orderBy("disaster_number")
dim = (all_disasters
       .withColumn("disaster_tk", row_number().over(w))
       .withColumn("version", lit(1))
       .withColumn("date_from", current_timestamp())
       .withColumn("date_to", lit(None).cast("timestamp")))
```
- `row_number()` daje stabilan, deterministiÄki `disaster_tk`.

#### 3.4 Reâ€‘ordering kolona
```python
COL_ORDER = ["disaster_tk", "version", "date_from", "date_to", "disaster_number", "incident_type", *BOOL_COLS]
```
- Garantira da Ä‡e schema biti uvijek identiÄnog reda â€“Â bitno kod `overwrite`.

#### 3.5 UNKNOWN red
```python
unknown_row = (0, 1, datetime.utcnow(), None, None, None, False, False, False, False)
final_df = spark.createDataFrame([unknown_row], schema=dim.schema).unionByName(dim)
```
- `disaster_tk = 0` i sve booleane `False`.

---

### 4.â€¯Pisanje u DWH
```python
final_df.write.jdbc(url, "disaster_dim", mode="overwrite", properties=props)
```
- Svako pokretanje puni svjeÅ¾u dimenziju.

---

## ğŸ“Š OÄekivani rezultat
```text
DISASTER_DIM redaka: â‰ˆÂ 4â€¯272   # 4â€¯271 stvarnih + 1 UNKNOWN
```

---

## ğŸ“ Bestâ€‘practice napomene
1. U buduÄ‡nosti ako FEMA promijeni `incident_type` za isti `disaster_number`, trebat Ä‡eÅ¡ SCDâ€‘2 logiku: zatvoriti stari red i insertati novi s verzijomÂ 2.
2. Boolean polja moÅ¾eÅ¡ pretvoriti i u `TINYINT(1)` u bazi radi kompatibilnosti.
3. `surrogate`Â â‰ Â `natural`: nikad ne koristi `disaster_number` direktno kao foreign key u factâ€‘tablici.

---

**Autor**: Data Engineering pomoÄ‡nik Â· lipanjÂ 2025.

