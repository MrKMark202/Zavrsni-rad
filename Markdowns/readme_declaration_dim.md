# ğŸ›ï¸ Spark ETLÂ â€“Â Kreiranje `Declaration_DIM` dimenzijske tablice

## ğŸ¯ Cilj
Dimenzija **Declaration_DIM** predstavlja svaki sluÅ¾beni FEMA dekret (`declaration_request_number`).

| Stupac                      | Opis                                                  |
|-----------------------------|-------------------------------------------------------|
| `declaration_tk`            | Surrogate kljuÄ (1Â â€¦Â n)                               |
| `version`                   | SCDâ€‘verzija (trenutno 1)                              |
| `date_from`                 | Datum kad je red postao aktivan                       |
| `date_to`                   | Datum kraja (NULLÂ = joÅ¡ aktivan)                      |
| `declaration_title`         | Naslov deklaracije                                    |
| `declaration_type`          | Vrsta (MAJORÂ DISASTER, EMERGENCY, â€¦)                  |
| `declaration_date`          | Datum izdavanja                                       |
| `declaration_request_number`| **Prirodni kljuÄ** â€“ jedinstveni ID deklaracije       |

> Postoji i **UNKNOWN red** (`declaration_tk = 0`) za â€nepovezaneâ€œ Äinjenice.

---

## ğŸ“‚ Datoteka: `declaration_dim.py`

### 1. Unicode izlaz
```python
sys.stdout.reconfigure(encoding="utf-8")
```
OsluÅ¡kuje da bi hrvatski znakovi (Ä, Ä‡, Å¾, â€¦) bili ispravno ispisani u terminalu.

---

### 2. Spark helper
```python
SparkSession.builder \
    .appName("US_DISASTER_SCHEMA") \
    .config("spark.sql.shuffle.partitions", "4") \
    .getOrCreate()
```
- ÄŒetiri shuffleâ€‘particije su optimalne za ~60k redova.

---

### 3. Funkcija `transform_declaration_dim()` â€“Â korak po korak

#### 3.1 *Extract* (baza + CSV)
```python
COLS = ["declaration_title", "declaration_type", "declaration_date", "declaration_request_number"]

db_df  = spark.read.jdbc(... '"Declaration"' ...) .select(*COLS)
csv_df = spark.read.csv(csv_path, header=True, inferSchema=True).select(*COLS)
```
- ÄŒitamo **ista** Äetiri stupca iz oba izvora.

#### 3.2 Union + dedup
```python
all_decl = (db_df.unionByName(csv_df)
            .dropna(subset=["declaration_request_number"])
            .dropDuplicates(["declaration_request_number"])
            .withColumn("declaration_date", to_date(col("declaration_date"))))
```
- **`dropDuplicates`** jamÄi jedan red po prirodnom kljuÄu (`declaration_request_number`).
- `to_date()` Äisti potencijalni timestamp â†’ Äuvamo samo datum.

#### 3.3 Surrogate kljuÄ + SCD kolone
```python
w = Window.orderBy("declaration_request_number")

dim = (all_decl
       .withColumn("declaration_tk", row_number().over(w))
       .withColumn("version", lit(1))
       .withColumn("date_from", current_timestamp())
       .withColumn("date_to", lit(None).cast("timestamp")))
```
- `row_number()` po `declaration_request_number` â‡’ deterministiÄki `declaration_tk`.

#### 3.4 Fiksni redoslijed kolona
```python
COL_ORDER = ["declaration_tk", "version", "date_from", "date_to", *COLS]
dim = dim.select(*COL_ORDER)
```
- PruÅ¾a predvidljiv `schema` pri `overwrite` pisanju.

#### 3.5 UNKNOWN red
```python
unknown = (0,1,datetime.utcnow(),None,None,None,None,None)
unknown_df = spark.createDataFrame([unknown], schema=dim.schema)
final_df  = unknown_df.unionByName(dim)
```
- RedÂ `0` hvata sve buduÄ‡e Äinjenice koje nemaju valjan `declaration_request_number`.

---

### 4. Pisanje u bazu
```python
final_df.write.jdbc(url, "declaration_dim", mode="overwrite", properties=props)
```
- Prepisuje (overwrite) staru verziju dimenzije svaki put.

---

## ğŸ“Š OÄekivani output
```text
DECLARATION_DIM redaka: â‰ˆÂ 4â€¯272   # 4â€¯271 stvarnih + 1 UNKNOWN
```
Broj redova ovisi o tome koliko jedinstvenih deklaracija ima u bazi + 20â€¯% CSVâ€‘a.

---

## ğŸ“ Bestâ€‘practice biljeÅ¡ke
1. Ako se u buduÄ‡nosti za isti `declaration_request_number` promijeni `declaration_title` â†’ preÄ‘i na SCDâ€‘TypeÂ 2.
2. Datum moÅ¾e ukljuÄivati vrijeme; `to_date()` uklanja vrijeme jer ga ovdje ne trebamo.
3. Ne zaboravi indeksirati stupac `declaration_request_number` u bazi radi brÅ¾ih joinâ€‘ova.

---

**Autor**: Data Engineering pomoÄ‡nik Â· lipanjÂ 2025.

